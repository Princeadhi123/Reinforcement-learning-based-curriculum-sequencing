# Reinforcement-Learning-Based Curriculum Sequencing

This repository implements **reinforcement-learning-based curriculum sequencing** for educational exercises.

Students are modeled in an **interactive environment** built from an educational assessment dataset. RL agents learn a policy for choosing the next exercise (or exercise group) for a student, with multi-objective rewards capturing:

- **Base performance** (scores / correctness)
- **Mastery** (improvement, remediation, spacing)
- **Motivation** (diversity, challenge)

Multiple RL algorithms are supported (tabular and deep), plus strong baseline policies, with a unified experiment runner and rich evaluation metrics.

---

## 1. Project Structure

At the top level:

- `preprocess_kt_data.py`  
  Script to preprocess the original Excel dataset into a **long-format CSV** used by the RL environment.
- `preprocessed_kt_data.csv`  
  Example preprocessed dataset (generated by the script above).
- `curriculum_sequencing_rl/`  
  Python package containing the RL environment, agents, training loop, configs, and evaluation.
- `all_models.csv`  
  Example metrics file saved from running a multi-model experiment.
- `command.txt`  
  Example command(s) used to run experiments.

Inside `curriculum_sequencing_rl/`:

- `main.py` – Command-line entry point for running experiments.
- `experiment_runner.py` – High-level orchestration of environment setup, baselines, RL training, and metrics aggregation.
- `evaluation.py` – Evaluation utilities and metrics (VPR, regret, speed, etc.).
- `compat.py` – Compatibility helpers for legacy code/experiments.
- `requirements.txt` – Python dependencies for this package.

Subpackages:

- `core/`
  - `config.py` – Dataclasses and loader for experiment configuration (environment + model configs).
  - `base.py` – Base classes for trainers and networks.
  - `factory.py` – Factory/registry for trainer classes.
  - `utils.py` (if present) – Common utilities (e.g., epsilon scheduler, seeding, device setup).
- `environment/`
  - `optimized_env.py` – Main interactive environment used for RL training.
- `agents/`
  - `q_learning_agent.py` – Tabular Q-learning agent.
  - `dqn_agent.py` – DQN agent.
  - `policy_gradient_agent.py` – Shared implementation for A2C/A3C/PPO.
  - `sarl_trainer.py` – Self-Adaptive RL (SARL) trainer built on top of DQN.
- `configs/`
  - `quick_multi_scal_adapt.yaml` – Multi-model configuration used to test accuracy, consistency, speed, scalability, and adaptability.
  - `sarl_quick.yaml` – Quick configuration for SARL-only smoke tests.
- `plot_poster.py`, `poster/` – Utilities and assets for visualization/poster plots.

---

## 2. Installation

### 2.1. Python version

The project targets a standard scientific Python stack (PyTorch, NumPy, pandas). A typical environment is:

- Python **3.8+** (3.8–3.11 should work as long as the dependency versions are satisfied).

### 2.2. Create and activate a virtual environment (recommended)

From the project root:

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / macOS
source .venv/bin/activate
```

### 2.3. Install dependencies

Install package requirements for the RL code:

```bash
pip install -r curriculum_sequencing_rl/requirements.txt
```

Contents of `curriculum_sequencing_rl/requirements.txt` (for reference):

- `torch` – neural networks and deep RL agents
- `numpy` – numerical computing
- `pandas` – data handling
- `matplotlib`, `seaborn` – plotting and analysis
- `pyyaml` – YAML configuration support
- `dataclasses-json` – (optional) dataclass serialization helpers
- Optional (commented out): `numba`, `scikit-learn` for performance/extra ML utilities

---

## 3. Data Preparation

### 3.1. Source data

The script `preprocess_kt_data.py` expects an **Excel file** containing student responses to a set of math/logic exercises. In the script, the default input path is hard-coded as:

```python
input_file = Path(r"c:\\Users\\pdaadh\\Desktop\\KT digiarvi\\V3_1b polished and anonymized (1).xlsx")
```

If you are running this project on a different machine or with a different dataset, **edit this path** accordingly.

### 3.2. Running preprocessing

From the project root:

```bash
python preprocess_kt_data.py
```

This will:

- Read the Excel file.
- Extract all exercise columns (`M3S*`).
- Map each exercise to:
  - A **fine-grained category** (e.g., `Rally1 2x3`).
  - A **coarse group** used by the RL environment (e.g., `Multiplication`, `Subtraction`, `Geometry`, `Coding`).
- Compute per-exercise metadata:
  - `order` – curriculum order index.
  - `max` – maximum achievable score per exercise.
  - `normalized_score` – score scaled to `[0, 1]`.
  - Additional features: grade, standardized sex, language match, missingness, and ability proxies.
- Save a **long-format** CSV file `preprocessed_kt_data.csv` in the project root.

If `preprocessed_kt_data.csv` is locked (e.g., opened in Excel), the script automatically writes to a timestamped fallback file (e.g., `preprocessed_kt_data_YYYYMMDD_HHMMSS.csv`).

### 3.3. Columns expected by the environment

The RL environment (`optimized_env.py`) expects at least the following columns in the preprocessed CSV:

- `student_id`, `exercise_id`
- `category`, `category_group`
- `order`
- `normalized_score`
- `grade`, `sex`
- `home_school_lang_match`
- `missing_all`, `missing_beginning30`, `missing_last50`

If any of these are missing, the environment will raise an error.

---

## 4. RL Environment Overview

The main environment class is `OptimizedInteractiveEnv` in `environment/optimized_env.py`.

### 4.1. State and action spaces

- **Actions**: choose the next exercise *category* (or `category_group`, depending on configuration).  
  Action indices map to sorted unique values of the chosen column.
- **State**: for each step, the state is a concatenation of:
  - One-hot encoding of the current action category.
  - 8 numeric features: `order_norm`, `normalized_score`, `grade_enc`, `sex_bin`, `home_school_lang_match`, `missing_all`, `missing_beginning30`, `missing_last50`.

The state dimension is `action_size + 8`.

### 4.2. Splits and episodes

- Students are split into **train / validation / test** sets using `train_ratio` and `val_ratio` in `EnvironmentConfig`.
- Each episode:
  - Samples a single student from the chosen split (`train` / `val` / `test`).
  - Simulates a sequence of exercise selections until all relevant exercises are consumed.

### 4.3. Reward shaping

Rewards are composed of three groups:

1. **Base reward** (performance)
   - Weighted combination of correctness and score via `reward_correct_w`, `reward_score_w`.
2. **Mastery shaping**
   - `rew_improve_w`: improvement over EMA baseline.
   - `rew_deficit_w`: deficit relative to mastery threshold `need_threshold`.
   - `rew_spacing_w`: spacing / revisiting intervals per category.
3. **Motivation shaping**
   - `rew_diversity_w`: diversity in recent exercise categories.
   - `rew_challenge_w`: proximity to target difficulty (`challenge_target` and `challenge_band`).

Each group is further scaled by **hybrid weights**:

- `hybrid_base_w`, `hybrid_mastery_w`, `hybrid_motivation_w`.

The environment also computes a **normalized shaped reward** (`reward_norm`) dividing by an effective maximum based on all weights. This is used for **speed metrics** (steps-to-threshold).

### 4.4. Invalid actions

If the agent selects a category with no remaining exercises for the current student:

- The environment auto-advances to a valid exercise.
- A penalty `invalid_penalty` is applied.
- The `info` dict marks `valid_action = False`.

---

## 5. Algorithms and Trainers

RL implementations live under `curriculum_sequencing_rl/agents`.

- **Q-Learning** (`q_learning_agent.py`)
  - Tabular agent using `QLearningConfig`.
- **DQN** (`dqn_agent.py`)
  - Deep Q-Network with replay buffer and target network.
  - Controlled by `DQNConfig`.
- **A2C / A3C / PPO** (`policy_gradient_agent.py`)
  - Policy gradient methods sharing common PG config (`PolicyGradientConfig`).
  - A2C (`A2CConfig`), A3C (`A3CConfig`), PPO (`PPOConfig`).
- **SARL (Self-Adaptive DQN)** (`sarl_trainer.py`)
  - Registered as trainer `"sarl"`.
  - Extends DQN with **adaptive curriculum and hyperparameters**:
    - Adapts hybrid reward weights toward target contribution shares.
    - Adapts epsilon based on invalid action rate and regret.
    - Adapts optimizer learning rate (plateau-based scheduling).
    - Adapts challenge parameters (`challenge_target`, `challenge_band`).

Trainers inherit from `BaseTrainer` (`core/base.py`) and are created via `TrainerFactory` (`core/factory.py`).

---

## 6. Configuration System

Configurations are managed via `core/config.py` using dataclasses and can be:

- Loaded from **YAML/JSON** (`Config.from_file`).
- Updated from **CLI arguments** (`Config.update_from_args`).

### 6.1. YAML structure

A typical config (see `configs/quick_multi_scal_adapt.yaml`) looks like:

```yaml
environment:
  data_path: "preprocessed_kt_data.csv"
  reward_correct_w: 0.0
  reward_score_w: 1.0
  # ... shaping weights, thresholds, hybrid weights, etc.

models: ["ql", "dqn", "a2c", "a3c", "ppo", "sarl"]

q_learning:
  # Q-LearningConfig fields
  epochs: 200
  alpha: 0.2
  # ...

dqn:
  # DQNConfig fields
  episodes: 200
  lr: 0.0002
  # ...

# a2c, a3c, ppo, sarl blocks follow similarly

include_chance: false
include_trivial: false
include_markov: false

seeds: [123, 456, 789]

evaluate_scalability: true
scalability_small_fraction: 0.5

evaluate_adaptability: true
adapt_pre_episodes: 80
adapt_post_episodes: 80
adapt_post_challenge_target: 0.8
adapt_post_challenge_band: 0.3

metrics_csv: "all_models.csv"
```

### 6.2. CLI overrides

`main.py` defines an extensive argument parser with groups:

- Environment: `--reward_correct_w`, `--reward_score_w`, `--action_on`, `--student_fraction`, etc.
- Reward shaping: `--rew_improve_w`, `--rew_deficit_w`, `--rew_spacing_w`, `--rew_diversity_w`, `--rew_challenge_w`, plus shaping hyperparameters.
- Hybrid weights: `--hybrid_base_w`, `--hybrid_mastery_w`, `--hybrid_motivation_w`.
- Evaluation: `--eval_episodes`, `--no_chance`, `--no_trivial`, `--no_markov`, demo options.
- Per-model blocks: `--ql_*`, `--dqn_*`, `--a2c_*`, `--a3c_*`, `--ppo_*`, `--sarl_*`.

If `--config` is provided:

- The YAML/JSON file is loaded into a `Config` object.
- Then `update_from_args` applies **non-`None` CLI arguments** to override configuration fields.
- For safety, some CLI defaults are explicitly neutralized when a config file is used so they do *not* clobber file values.

---

## 7. Running Experiments

All experiments are launched through `curriculum_sequencing_rl.main`.

### 7.1. Basic usage (config file)

From the project root:

```bash
python -m curriculum_sequencing_rl.main \
  --config curriculum_sequencing_rl/configs/quick_multi_scal_adapt.yaml
```

This will:

- Load `preprocessed_kt_data.csv` (or the `data_path` you specify) via the environment config.
- Instantiate `ExperimentRunner` with the given `Config`.
- Optionally run baselines (Chance, Trivial, Markov), depending on `include_*` flags.
- Train and evaluate all models listed in `models`.
- Print a detailed metrics summary to the console.
- Save results to `metrics_csv` (e.g., `all_models.csv`).

### 7.2. Quick SARL-only run

```bash
python -m curriculum_sequencing_rl.main \
  --config curriculum_sequencing_rl/configs/sarl_quick.yaml
```

This configuration:

- Runs only `sarl` (Self-Adaptive RL) with a smaller number of episodes.
- Disables heavy cross-evaluations (no scalability/adaptability) to keep runs short.
- Optionally skips baselines for speed.

### 7.3. Running without a config file

You can also run with CLI-only configuration:

```bash
python -m curriculum_sequencing_rl.main \
  --data preprocessed_kt_data.csv \
  --models ql,dqn \
  --reward_score_w 1.0 --reward_correct_w 0.0 \
  --ql_epochs 50 --dqn_episodes 100
```

In this mode:

- `--data` sets `environment.data_path`.
- `--models` is parsed into a list like `["ql", "dqn"]`.
- Per-model hyperparameters are taken from the CLI defaults unless overridden.

### 7.4. Demo rollouts

To print human-readable sample trajectories:

```bash
python -m curriculum_sequencing_rl.main \
  --config curriculum_sequencing_rl/configs/sarl_quick.yaml \
  --demo --demo_episodes 2 --demo_steps 12 --demo_mode test
```

This triggers `print_sample_rollouts` in `evaluation.py` for each model/baseline.

---

## 8. Evaluation Metrics

Evaluation is handled by `evaluation.py`, primarily `eval_policy_interactive_metrics` and helpers.

Key metrics per policy:

- **Reward metrics**
  - `reward`: average shaped reward per step.
  - `reward_base`: average base reward (pre-shaping).
  - `reward_shaping`: average combined shaping reward.
  - `reward_base_contrib`, `reward_mastery`, `reward_motivation`: hybrid-weighted contributions.
  - `reward_norm`: normalized shaped reward (0–1-ish).
- **Shaping terms**
  - `term_improve`, `term_deficit`, `term_spacing`, `term_diversity`, `term_challenge`.
- **Validity and regret**
  - `vpr`: valid pick rate – fraction of steps where the selected action was valid.
  - `regret`: average instantaneous regret vs. the best remaining immediate reward.
  - `regret_ratio`: regret relative to the best possible reward.
- **Speed metrics** (optional, if `speed_threshold_norm` is set)
  - `speed_steps_to_threshold_mean` / `median`: steps to first reach `reward_norm >= threshold`.
  - `speed_success_rate`: fraction of episodes that ever reached the threshold.
- **Episodic returns**
  - `ep_return_mean`, `ep_return_std`: mean and std of per-episode returns.

`ExperimentRunner` further aggregates results across seeds and models, computing **axis scores** (0–100):

- `axis_accuracy`
- `axis_consistency`
- `axis_speed`
- `axis_scalability`
- `axis_adaptability`

These are derived from normalized episodic returns and ratios (e.g., small/base, post/pre), and exported into the CSV.

Console summaries (in `_print_results_summary`) show a concise view of:

- Shaped vs. base rewards.
- Normalized rewards and regret.
- Hybrid contribution shares.
- Basic speed statistics.

---

## 9. CSV Outputs

If `metrics_csv` is set in the config, `ExperimentRunner` writes results to that CSV. The header includes:

- Core metrics listed above (reward, vpr, regret, speed, episodic returns).
- Variant labels: `variant` (e.g., `base`, `scalability_small`, `adapt_pre`, `adapt_post`, `aggregate`).
- Aggregated axis scores: `axis_accuracy`, `axis_consistency`, `axis_speed`, `axis_scalability`, `axis_adaptability`.
- Environment configuration snapshot: reward weights, shaping weights, hybrid weights, `student_fraction`, etc.

This makes it easy to analyze experiments later with pandas/NumPy.

---

## 10. Reproducibility

Reproducibility controls include:

- `environment.seed` – base seed for the environment and student sampling.
- Model-specific `seed` fields in `TrainingConfig` derivatives.
- `seeds` list in `ExperimentConfig` – if provided, experiments are repeated across multiple seeds and aggregated.

`ExperimentRunner` uses these seeds consistently so that runs can be reproduced.

---

## 11. Extending the Project

Here are some common extension points:

- **Adding a new RL algorithm**
  1. Implement a new trainer subclass of `BaseTrainer`.
  2. Register it in `core/factory.py` using the `@register_trainer("name")` decorator.
  3. Add a corresponding config dataclass if needed, and expose CLI flags.

- **Changing the reward shaping scheme**
  - Modify `_compute_shaping_terms` and related logic in `optimized_env.py`.
  - Add new weights/parameters to `EnvironmentConfig` and config files.

- **Using a different dataset**
  - Adapt `preprocess_kt_data.py` to your schema while preserving the core columns expected by the environment.
  - Point `environment.data_path` to your new CSV.

---

## 12. Quick Start Summary

1. **Prepare environment**
   - Create a virtual env and `pip install -r curriculum_sequencing_rl/requirements.txt`.
2. **Preprocess data** (if needed)
   - Edit `preprocess_kt_data.py` to point to your Excel file.
   - Run `python preprocess_kt_data.py` to generate `preprocessed_kt_data.csv`.
3. **Run an experiment**
   - Multi-model eval:  
     `python -m curriculum_sequencing_rl.main --config curriculum_sequencing_rl/configs/quick_multi_scal_adapt.yaml`
   - SARL quick check:  
     `python -m curriculum_sequencing_rl.main --config curriculum_sequencing_rl/configs/sarl_quick.yaml`
4. **Inspect results**
   - Check console metrics.
   - Load `all_models.csv` or `sarl_quick.csv` in pandas or a spreadsheet tool for analysis.

This README should give you enough context to run, analyze, and extend the reinforcement-learning-based curriculum sequencing experiments in this repository.
